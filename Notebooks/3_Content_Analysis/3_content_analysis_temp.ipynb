{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Function to extract content from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email Content:\n",
      "This is the link: Displayedlinl.com <https://www.wikihow.com/Main-Page>\n",
      "\n",
      "This is the txt attachment:\n",
      "\n",
      "This is an image !\n",
      "[image: how_emails_work.png]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import email\n",
    "import os\n",
    "\n",
    "def extract_content(msg):\n",
    "    content = None\n",
    "\n",
    "    if msg.is_multipart():\n",
    "        for part in msg.walk():\n",
    "            content_type = part.get_content_type()\n",
    "            content_disposition = str(part.get(\"Content-Disposition\"))\n",
    "\n",
    "            # Ignore any attachments\n",
    "            if content_type == \"text/plain\" and \"attachment\" not in content_disposition:\n",
    "                content = part.get_payload(decode=True).decode(part.get_content_charset())\n",
    "                break\n",
    "    else:\n",
    "        content = msg.get_payload(decode=True).decode(msg.get_content_charset())\n",
    "\n",
    "    return content\n",
    "\n",
    "def get_email_content(eml_file_path):\n",
    "    with open(eml_file_path, 'r') as eml_file:\n",
    "        msg = email.message_from_file(eml_file)\n",
    "\n",
    "        # Extract email content\n",
    "        content = extract_content(msg)\n",
    "        return content\n",
    "\n",
    "# Example usage\n",
    "eml_file_path = '../Example_emails/email_1.eml'\n",
    "if os.path.exists(eml_file_path):\n",
    "    email_content = get_email_content(eml_file_path)\n",
    "    if email_content:\n",
    "        print(\"Email Content:\")\n",
    "        print(email_content)\n",
    "    else:\n",
    "        print(\"No email content found.\")\n",
    "else:\n",
    "    print(\"File not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Languaged based Analysis - identifying spelling mistakes, generic greetings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install textblob\n",
    "# pip install word_tokenize\n",
    "# pip install nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Generic Greetings Detected': ['hello'], 'Spelling Mistakes Detected': ['Hello', 'We', 'This', 'Please', 'Thank'], 'Urgent Language Occurrence Detected': ['urgent', 'as soon as possible'], 'Personal Information Inquiry Detected': ['name', 'address', 'phone']}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def analyze_text(text):\n",
    "    # Initialize dictionary to store results\n",
    "    analysis_results = {\n",
    "        \"Generic Greetings Detected\": [],\n",
    "        \"Spelling Mistakes Detected\": [],\n",
    "        \"Urgent Language Occurrence Detected\": [],\n",
    "        \"Personal Information Inquiry Detected\": []\n",
    "    }\n",
    "\n",
    "    # Detecting generic greetings\n",
    "    generic_greetings = ['hello', 'hi', 'hey', 'good morning', 'good afternoon', 'good evening']\n",
    "    greetings_detected = [word for word in word_tokenize(text.lower()) if word in generic_greetings]\n",
    "    analysis_results[\"Generic Greetings Detected\"] = greetings_detected\n",
    "\n",
    "    # Finding spelling and grammar mistakes\n",
    "    blob = TextBlob(text)\n",
    "    spelling_mistakes = [word for word in blob.words if word.lower() != TextBlob(word).correct()]\n",
    "    analysis_results[\"Spelling Mistakes Detected\"] = spelling_mistakes\n",
    "\n",
    "    # Finding the occurrence of urgent language\n",
    "    urgent_keywords = ['urgent', 'emergency', 'important', 'asap', 'as soon as possible']\n",
    "    urgent_occurrence = [keyword for keyword in urgent_keywords if keyword in text.lower()]\n",
    "    analysis_results[\"Urgent Language Occurrence Detected\"] = urgent_occurrence\n",
    "\n",
    "    # Finding if the text asks personal information\n",
    "    personal_info_regex = r'\\b(?:name|address|phone|email|social security|credit card|password)\\b'\n",
    "    personal_info_detected = re.findall(personal_info_regex, text.lower())\n",
    "    analysis_results[\"Personal Information Inquiry Detected\"] = personal_info_detected\n",
    "\n",
    "    return analysis_results\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"\n",
    "Hello there! We hope you're doing well.\n",
    "This is an urgent message regarding your account.\n",
    "Please provide your name, address, and phone number as soon as possible.\n",
    "Thank you and have a great day!\n",
    "\"\"\"\n",
    "results = analyze_text(text)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Context-based Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This works - better to test it directly on Google Colab\n",
    "from transformers import pipeline\n",
    "\n",
    "def classify(text):\n",
    "    classifier = pipeline(\"zero-shot-classification\")\n",
    "    candidate_labels = [\"prizes and giveaways\", \"job opportunities\", \"banking\", \"update password request\"]\n",
    "    result = classifier(text, candidate_labels)\n",
    "    labels = result[\"labels\"]\n",
    "    scores = result[\"scores\"]\n",
    "    for label, score in zip(labels, scores):\n",
    "        print(f\"{label}: {score}\")\n",
    "\n",
    "# Example text\n",
    "text = \"\"\"\n",
    "This is a new job for you!\n",
    "\"\"\"\n",
    "\n",
    "classify(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update password request: 0.9863448143005371\n",
      "banking: 0.005188442301005125\n",
      "job opportunities: 0.0044320616871118546\n",
      "prizes and giveaways: 0.0040346370078623295\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def classify(text):\n",
    "    headers = {\"Authorization\": \"Bearer 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\"}\n",
    "    payload = {\n",
    "        \"inputs\": text,\n",
    "        \"parameters\": {\n",
    "            \"candidate_labels\": [\"prizes and giveaways\", \"job opportunities\", \"banking\", \"update password request\"]\n",
    "        }\n",
    "    }\n",
    "    response = requests.post(\"https://api-inference.huggingface.co/models/typeform/distilbert-base-uncased-mnli\", headers=headers, json=payload)\n",
    "    result = response.json()\n",
    "    labels = result[\"labels\"]\n",
    "    scores = result[\"scores\"]\n",
    "    for label, score in zip(labels, scores):\n",
    "        print(f\"{label}: {score}\")\n",
    "\n",
    "# Example text\n",
    "text = \"\"\"\n",
    "High-severity alert! Please look at your account and update !\n",
    "\n",
    "Hello there ! Please find the attached document for your reference. This is an urgent request, we need to get this done as soon as possible as your credentials have been tampered with! Please provide your name, address, and phone number. Please hurry!\n",
    "\n",
    "You can access your credentials directly on this website: www.outlook.com/update-user-link. \n",
    "\n",
    "If you have any questions, you can refer to www.outlook.com !\n",
    "\n",
    "PS: open the document to update your credentials faster !! \n",
    "\"\"\"\n",
    "\n",
    "classify(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Spam / Not Spam classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5728, 2)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/OmkarPathak/Playing-with-datasets/master/Email%20Spam%20Filtering/emails.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8516579406631762\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pickle\n",
    "\n",
    "# 1. Load the dataset\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/OmkarPathak/Playing-with-datasets/master/Email%20Spam%20Filtering/emails.csv')\n",
    "\n",
    "# 2. Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['spam'], test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Create a pipeline with TF-IDF vectorizer and a classifier (e.g., Multinomial Naive Bayes)\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "# 4. Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 5. Evaluate the model\n",
    "accuracy = pipeline.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# 6. Export model and TF-IDF vectorizer using pickle\n",
    "with open('spam_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(pipeline, f)\n",
    "\n",
    "# Export TF-IDF vectorizer separately\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(pipeline.named_steps['tfidf'], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_subject(subject, tfidf_vectorizer):\n",
    "    # Importing the saved model\n",
    "    with open('spam_classifier_model.pkl', 'rb') as f:\n",
    "        loaded_model = pickle.load(f)\n",
    "\n",
    "    # Transform the subject into a vector\n",
    "    print(subject)\n",
    "    example_vector = tfidf_vectorizer.transform([subject]).toarray()\n",
    "\n",
    "    # Prediction\n",
    "    prediction = loaded_model.predict(example_vector)\n",
    "    return \"Spam\" if prediction[0] == 1 else \"Not spam\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Hello there ! Please find the attached document for your reference. This is an urgent request, we need to get this done as soon as possible as your credentials have been tampered with! Please provide your name, address, and phone number. Please hurry!\" is spam.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MultinomialNB was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load TF-IDF vectorizer\n",
    "with open('tfidf.pkl', 'rb') as f:\n",
    "    tfidf_vectorizer = pickle.load(f)\n",
    "\n",
    "# Load the model\n",
    "with open('spam_classifier_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Example sentence to test\n",
    "# example_sentence = \"Get a free cruise trip! Claim now!\"\n",
    "example_sentence = \"Hello there ! Please find the attached document for your reference. This is an urgent request, we need to get this done as soon as possible as your credentials have been tampered with! Please provide your name, address, and phone number. Please hurry!\"\n",
    "\n",
    "# Transform the example sentence using the TF-IDF vectorizer\n",
    "example_vector = tfidf_vectorizer.transform([example_sentence]).toarray()\n",
    "\n",
    "# Predict whether the example sentence is spam or not\n",
    "prediction = model.predict(example_vector)\n",
    "\n",
    "# Print the prediction\n",
    "if prediction[0] == 1:\n",
    "    print(f'\"{example_sentence}\" is spam.')\n",
    "else:\n",
    "    print(f'\"{example_sentence}\" is not spam.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
